The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading model ...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:01<01:01, 61.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:29<00:00, 41.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:29<00:00, 44.58s/it]
/rscratch/adityatomar/anaconda3/envs/myenv/lib/python3.11/site-packages/transformers/modeling_utils.py:4779: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
/rscratch/adityatomar/KVQuant/quant/kvquant/simquant_module_quantizer.py:612: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.outlier_threshold_upper = torch.tensor(quantizer[0]).cuda().flatten().half()
/rscratch/adityatomar/KVQuant/quant/kvquant/simquant_module_quantizer.py:613: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.outlier_threshold_lower = torch.tensor(quantizer[1]).cuda().flatten().half()
Done.
Evaluating ...
Batch 0 of 21
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Batch 1 of 21
Batch 2 of 21
Batch 3 of 21
Batch 4 of 21
Batch 5 of 21
Batch 6 of 21
Batch 7 of 21
Batch 8 of 21
Batch 9 of 21
Batch 10 of 21
Batch 11 of 21
Batch 12 of 21
Batch 13 of 21
Batch 14 of 21
Batch 15 of 21
Batch 16 of 21
Batch 17 of 21
Batch 18 of 21
Batch 19 of 21
Batch 20 of 21
5.492496013641357
