The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading model ...
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:42<01:25, 42.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:19, 19.65s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:50<00:00, 12.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:50<00:00, 16.69s/it]
/rscratch/adityatomar/anaconda3/envs/myenv/lib/python3.11/site-packages/transformers/modeling_utils.py:4779: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
/rscratch/adityatomar/KVQuant/quant/kvquant/simquant_module_quantizer.py:612: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.outlier_threshold_upper = torch.tensor(quantizer[0]).cuda().flatten().half()
/rscratch/adityatomar/KVQuant/quant/kvquant/simquant_module_quantizer.py:613: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.outlier_threshold_lower = torch.tensor(quantizer[1]).cuda().flatten().half()
Done.
Evaluating ...
Batch 0 of 32
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Batch 1 of 32
Batch 2 of 32
Batch 3 of 32
Batch 4 of 32
Batch 5 of 32
Batch 6 of 32
Batch 7 of 32
Batch 8 of 32
Batch 9 of 32
Batch 10 of 32
Batch 11 of 32
Batch 12 of 32
Batch 13 of 32
Batch 14 of 32
Batch 15 of 32
Batch 16 of 32
Batch 17 of 32
Batch 18 of 32
Batch 19 of 32
Batch 20 of 32
Batch 21 of 32
Batch 22 of 32
Batch 23 of 32
Batch 24 of 32
Batch 25 of 32
Batch 26 of 32
Batch 27 of 32
Batch 28 of 32
Batch 29 of 32
Batch 30 of 32
Batch 31 of 32
6.746776580810547
